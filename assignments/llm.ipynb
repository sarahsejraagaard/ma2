{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**AI & Machine Learning (KAN-CINTO4003U) - Copenhagen Business School | Spring 2025**\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Part III: LLM\n",
    "\n",
    "Please see the description of the assignment in the README file (section 3) <br>\n",
    "**Guide notebook**: [guides/llm_guide.ipynb](guides/llm_guide.ipynb)\n",
    "\n",
    "\n",
    "***\n",
    "\n",
    "<br>\n",
    "\n",
    "* Note that you should report results using a classification report. \n",
    "\n",
    "* Also, remember to include some reflections on your results: how do they compare with the results from Part I, BoW?, and part II, BERT? Are there any hyperparameters or prompting techniques that are particularly important?\n",
    "\n",
    "* You should follow the steps given in the `llm_guide` notebook\n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports for the project\n",
    "\n",
    "import pandas as pd\n",
    "from decouple import config\n",
    "from ibm_watsonx_ai import APIClient\n",
    "from ibm_watsonx_ai import Credentials\n",
    "from ibm_watsonx_ai.foundation_models import ModelInference\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the environment variables using python-decouple\n",
    "# The .env file should be in the root of the project\n",
    "# The .env file should NOT be committed to the repository\n",
    "\n",
    "WX_API_KEY = config('WX_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Working Directory: /Users/sarahaagaard/Documents/sarah/HA IT/4. år/8. semester/Artificial Intelligence and Machine Learning/AIML25/mas/ma2\n",
      "Files in Directory: ['assignments', 'LICENSE', 'environment.yml', 'README.md', '.gitignore', '.env', 'guides', 'test.hf', '.env.example', '.git', '.vscode', 'data']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "print(\"Current Working Directory:\", os.getcwd())\n",
    "print(\"Files in Directory:\", os.listdir())  # Check if .env is listed here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ API Key Loaded Successfully!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Retrieve the API key\n",
    "api_key = os.getenv(\"WX_API_KEY\")\n",
    "\n",
    "if api_key:\n",
    "    print(\"✅ API Key Loaded Successfully!\")\n",
    "else:\n",
    "    print(\"❌ Error: API Key not found. Check your .env file.\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Connecting to the WatsonX.ai Credentials API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "credentials = Credentials(\n",
    "    url = \"https://us-south.ml.cloud.ibm.com\", # set the URL to dallas\n",
    "    api_key = WX_API_KEY\n",
    ")\n",
    "\n",
    "client = APIClient(\n",
    "    credentials=credentials, \n",
    "    project_id=\"e2fff713-c98f-4236-8836-47cbe38cd27d\" # insert my unique project ID\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Testing the connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = ModelInference(\n",
    "    api_client=client,\n",
    "    model_id=\"ibm/granite-13b-instruct-v2\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+----------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "| PARAMETER             | TYPE                                   | EXAMPLE VALUE                                                                                                                             |\n",
      "+=======================+========================================+===========================================================================================================================================+\n",
      "| decoding_method       | str, TextGenDecodingMethod, NoneType   | sample                                                                                                                                    |\n",
      "+-----------------------+----------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "| length_penalty        | dict, TextGenLengthPenalty, NoneType   | {'decay_factor': 2.5, 'start_index': 5}                                                                                                   |\n",
      "+-----------------------+----------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "| temperature           | float, NoneType                        | 0.5                                                                                                                                       |\n",
      "+-----------------------+----------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "| top_p                 | float, NoneType                        | 0.2                                                                                                                                       |\n",
      "+-----------------------+----------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "| top_k                 | int, NoneType                          | 1                                                                                                                                         |\n",
      "+-----------------------+----------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "| random_seed           | int, NoneType                          | 33                                                                                                                                        |\n",
      "+-----------------------+----------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "| repetition_penalty    | float, NoneType                        | 2                                                                                                                                         |\n",
      "+-----------------------+----------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "| min_new_tokens        | int, NoneType                          | 50                                                                                                                                        |\n",
      "+-----------------------+----------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "| max_new_tokens        | int, NoneType                          | 1000                                                                                                                                      |\n",
      "+-----------------------+----------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "| stop_sequences        | list, NoneType                         | 200                                                                                                                                       |\n",
      "+-----------------------+----------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "| time_limit            | int, NoneType                          | 600000                                                                                                                                    |\n",
      "+-----------------------+----------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "| truncate_input_tokens | int, NoneType                          | 200                                                                                                                                       |\n",
      "+-----------------------+----------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "| return_options        | dict, ReturnOptionProperties, NoneType | {'input_text': True, 'generated_tokens': True, 'input_tokens': True, 'token_logprobs': True, 'token_ranks': False, 'top_n_tokens': False} |\n",
      "+-----------------------+----------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "| include_stop_sequence | bool, NoneType                         | True                                                                                                                                      |\n",
      "+-----------------------+----------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "| prompt_variables      | dict, NoneType                         | {'doc_type': 'emails', 'entity_name': 'Golden Retail'}                                                                                    |\n",
      "+-----------------------+----------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "from ibm_watsonx_ai.foundation_models.schema import TextGenParameters\n",
    "\n",
    "TextGenParameters.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import classification_report \n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "splits = {'train': 'data/train-00000-of-00001.parquet', 'test': 'data/test-00000-of-00001.parquet'}\n",
    "# train = pd.read_parquet(\"hf://datasets/fancyzhx/ag_news/\" + splits[\"train\"])\n",
    "test = pd.read_parquet(\"hf://datasets/fancyzhx/ag_news/\" + splits[\"test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((760, 2),)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_map = {\n",
    "    0: 'World',\n",
    "    1: 'Sports',\n",
    "    2: 'Business',\n",
    "    3: 'Sci/Tech'\n",
    "}\n",
    "\n",
    "def preprocess(df: pd.DataFrame, frac = 1e-2, label_map = label_map, seed=42) -> pd.DataFrame:\n",
    "    return  (\n",
    "        df\n",
    "        .assign(label=lambda x: x['label'].map(label_map))\n",
    "        [lambda df: df['label'].isin(label_map.values())]\n",
    "        .groupby('label')\n",
    "        .apply(lambda x: x.sample(frac=frac, random_state=seed))\n",
    "        .reset_index(drop=True)\n",
    "\n",
    "    )\n",
    "\n",
    "# train_df = preprocess(train, frac=0.01)\n",
    "test_df = preprocess(test, frac=0.1)\n",
    "\n",
    "# clear up some memory by deleting the original dataframes\n",
    "# del train\n",
    "del test\n",
    "\n",
    "test_df.shape, # train_df.shape, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "Business    190\n",
       "Sci/Tech    190\n",
       "Sports      190\n",
       "World       190\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.label.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Set model parameers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "PARAMS = TextGenParameters(\n",
    "    temperature=0,              # Higher temperature means more randomness - In this case we don't want randomness\n",
    "    max_new_tokens=10,          # Maximum number of tokens to generate\n",
    "    stop_sequences=[\".\", \"\\n\"], # Stop generating text when these sequences are encountered\n",
    ")\n",
    "\n",
    "model = ModelInference(\n",
    "    api_client=client,\n",
    "    model_id=\"ibm/granite-13b-instruct-v2\",  # We could also try a larger model!\n",
    "    params=PARAMS\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Create a system prompt - change this!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT = \"\"\"\n",
    "You are an advanced news categorization model. Your task is to classify a given news article into one of four categories: \n",
    "\n",
    "CATEGORIES:\n",
    "- World: Covers international news, politics, diplomacy, conflicts, and global events.\n",
    "- Sports: Includes news about athletic competitions, teams, players, and sporting events.\n",
    "- Business: Encompasses financial markets, companies, economic trends, and corporate news.\n",
    "- Sci/Tech: Relates to scientific discoveries, technological advancements, and industry innovations.\n",
    "\n",
    "To classify the article correctly, follow this step-by-step reasoning:\n",
    "\n",
    "1. Identify the main subject of the article. Is it discussing a country's affairs, an economic trend, a sports event, or a scientific/technological breakthrough?\n",
    "2. Look for keywords and context clues. Does it mention political leaders, stock markets, athletes, or emerging technologies?\n",
    "3. Determine the dominant theme. If the article covers multiple aspects, prioritize the primary focus.\n",
    "4. Assign the most appropriate category based on the above reasoning.\n",
    "\n",
    "TEXT:\n",
    "{text}\n",
    "\n",
    "Choose *ONLY* World, Sports, Business OR Sci/Tech! Based on the analysis, the correct category is:\n",
    "\n",
    "Category:\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chain of though:\n",
    "# Think step by step about this. Show your reasoning. The, on a new line choose *ONLY* World, Sports, Business OR SciTech!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Changes to the model:\n",
    "- The task is framed as something the model \"is\" (a news categorization model), which might help it better focus on the task. Might make the instructions clearer.\n",
    "- Added detailed descriptions of each category – This provides the model with clearer distinctions between the four categories\n",
    "- Introduced a step-by-step reasoning process\n",
    "- Ensured that the model outputs only one of the 4 categories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Generate predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 760/760 [04:58<00:00,  2.55it/s]\n"
     ]
    }
   ],
   "source": [
    "CATEGORIES = \"- \" + \"\\n- \".join(test_df[\"label\"].unique())  # Create a string with all categories\n",
    "\n",
    "predictions = []\n",
    "\n",
    "for text in tqdm(test_df[\"text\"]):\n",
    "\n",
    "    # format the prompt with the categories and the text\n",
    "    prompt = SYSTEM_PROMPT.format(text=text)\n",
    "    \n",
    "    # generate the response from the model\n",
    "    response = model.generate(prompt)\n",
    "\n",
    "    #     answer = response.split(\"\\n\")[-1] # this should be added if i did chain-of-though\n",
    "\n",
    "    # extract the generated text from the response\n",
    "    prediction = response[\"results\"][0][\"generated_text\"].strip()\n",
    "\n",
    "    # append the prediction to the list of predictions\n",
    "    predictions.append(prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Evaluate performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Business       0.47      0.95      0.63       190\n",
      "    Sci/Tech       0.92      0.36      0.52       190\n",
      "      Sports       0.96      0.69      0.80       190\n",
      "       World       0.84      0.73      0.78       190\n",
      "\n",
      "    accuracy                           0.68       760\n",
      "   macro avg       0.80      0.68      0.68       760\n",
      "weighted avg       0.80      0.68      0.68       760\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(test_df.label, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Model evaluation and comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BoW Model:\n",
    "- Training Performance: 100% accuracy\n",
    "- Test Performance: 83% accuracy\n",
    "\n",
    "BERT Model:\n",
    "- Training Performance: 99% accuracy\n",
    "- Test Performance: 79% accuracy\n",
    "\n",
    "LLM (Large Language Model) Performance:\n",
    "- Accuracy: 68%\n",
    "\n",
    "The LLM model demonstrated the lowest overall accuracy (68%) compared to BoW and BERT. However, it outperformed BoW on the recall for the category Business.\n",
    " The LLM performed well in precision for Sports but struggled with some other categories, especially Space Sci/Tech, which received a 0.52 in f1-score.\n",
    "\n",
    "The LLM prompt plays a significant role in performance. A well-designed prompt with clear categories and instructions leads to more accurate predictions. In my case, the prompt might be too\n",
    "Using examples in the prompt, rather than just listing categories, could help the model understand the types of articles that belong to each category.\n",
    "\n",
    "In conclusion,  the LLM performse worse than BoW and BERT model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiml25-ma2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
